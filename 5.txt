1. multiple view geometry
	a field studying the relationship between cameras and features when there are correspondences between many images that are taken from varying viewpoints
	basic concepts
		epipolar geometry
			x2.T F x1 = 0
				epipolar constraint
				x1 and x2 are the projection of the same 3D point in different cameras
				F, fundamental matrix
					decided by the K,R,t of the two cameras
					usually we set the origin and coordinate axis to align with the first camera, so that
						P1 = K1[I|0]
						and P2 can be calculated using F and P1
				this means that camera matrices can be recovered from F, which can be computed from point correspondences
		computing F -the eight point algorithm
			# see sfm.compute_fundamental(x1, x2)
		computing epipole from F
			# see sfm.compute_epipole(F)
			# see basic_examples.example_compute_and_plot_epipole()
		triangulation
			recover the 3d position of a point using projections and camera matrices
			# see sfm.triangulate_point(x1,x2,P1,P2)
			# see basic_examples.example_triangulation()
		computing camera matrix from 2d-3d correspondences
			the reverse of triangulation
			# see sfm.compute_P(x, X)
			# see basic_examples.example_compute_P_from_points()
		computing the camera matrix from F
			the uncalibrated case --projective reconstruction
				without knowing K, the camera matrix can only be retrieved up to a projective transformation
					 means that angles and distances are not respected
				# see sfm.compute_P_from_fundamental(F)
			the calibrated case --metrix reconstruction
				The fundamental matrix for calibration-normalized coordinates is called the essential matrix
				The camera matrices recovered from an essential matrix respect metric relationships but there are four possible solutions
					Only one of them has the scene in front of both cameras
					how to pick from them?
						# see example_3d_reconstruction.py
				# see sfm.compute_P_from_essential(E)

	using a sample data set
		http://www.robots.ox.ac.uk/~vgg/data/data-mview.html
		the data set contains 2D information of each picture, 3D information, correspondences and also camera matrices
			# see basic_examples.py

	multiple view reconstruction (structure from motion, sfm)
		general idea
			Detect feature points and match them between the two images
			Compute the fundamental matrix from the matches
				use RANSAC to make it robust
				# see sfm.F_from_ransac()
			Compute the camera matrices from the fundamental matrix (known K)
			Triangulate the 3D points
		# see example_3d_reconstruction.py
	
	more
		with more than 2 views, the reconstruction will be more accurate and more detailed
		but when we incrementally add more views, the error accumulates
			the approach to minimize this is called Bundle Adjustment
		for those uncalibrated cameras, it's possible to compute the calibration from the image features
			Self-calibration

	stereo images
		a special case of multi-view imaging
			where two cameras are observing the same scene with only a horizontal displacement between the cameras
		Stereo reconstruction (sometimes called dense depth reconstruction) 
			is the problem of recovering a depth map (or, inversely, a disparity map) 
				where the depth (or disparity) for each pixel in the image is estimated
		Z = fb/(xl-xr)	# using similar triangles
			f, focal length
			b, the distance between the two camera centers
			(xl-xr), the distance between the related points on the two views
		computing disparity maps (depth map)
			# see stereo.py
			general idea
				try different depths in a given range to find the best depth for each pixel

2. clustering images --to find groups of similar images
	K-means clustering
		a simple clustering algorithm trying to put the input data into k clusters
			a. initialize k centroids randomly or with some guess
			b. assign each data point to the class of the nearest controid
			c. update the centroids as the average of all data points assigned to that class
			d. repeat b and c until convergence (stable)
		tries to minimize the total within-class variance
			does not guarantee the best solution is found
		to avoid the effects of choosing bad initial centroids
			it's often run several times with different initial centroids and then choose the one with minimum variance
			(the SciPy implementation does this for us)
		the main drawback is the number of clusters needs to be decided beforehand
			a inapporpriate choice leads to poor results

		SciPy implementation
			# from scipy.cluster.vq import *
			centroids,variance = kmeans(features,2)		# features is the data set, one row for each data point
									# 2 is K
			code,distance = vq(features,centroids)		# code is an array indicating which cluster each data point is in
			for k in range(2):
				ind = where(code==k)[0]			# get the index of data points which are inside cluster k

		# see k_means.example_K_means_fonts()
			we use the projection coefficients after projecting the image data on the 20 first pricipal components obtained via pca
				as descriptor vector for each image
				(an array of 20 values for one image then)
			try with different number of principal components and different number of clusters
		# see k_means.example_K_means_pixel()
			we just naively apply k-means on the pixel values
				not really meaningful, just a basic example

